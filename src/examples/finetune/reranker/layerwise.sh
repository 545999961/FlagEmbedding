torchrun --nproc_per_node 8 \
-m src.finetune.reranker.decoder_only.layerwise \
--output_dir ./test \
--model_name_or_path /share/chaofan/models/minicpm-2b-fp32-dpo \
--train_data /share/chaofan/dataset/mteb_data_new_score/en/fiqa.jsonl \
--cache_dir /share/shared_models \
--learning_rate 2e-4 \
--num_train_epochs 1 \
--max_steps 5 \
--per_device_train_batch_size 1 \
--gradient_accumulation_steps 16 \
--dataloader_drop_last True \
--gradient_checkpointing \
--query_max_len 512 \
--passage_max_len 512 \
--train_group_size 16 \
--logging_steps 1 \
--save_total_limit 50 \
--fp16 \
--dataloader_drop_last True \
--weight_decay 0.01 \
--cache_path ./data \
--use_lora True \
--lora_rank 32 \
--lora_alpha 64 \
--use_flash_attn True \
--target_modules q_proj k_proj v_proj o_proj linear_head \
--save_merged_lora_model True \
--model_type decoder \
--deepspeed /share/chaofan/code/stage/stage1.json \
--model_type from_raw_model \
--start_layer 8 \
--head_multi True \
--head_type simple \
--trust_remote_code True